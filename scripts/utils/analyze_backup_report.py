#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –æ—Ç—á–µ—Ç–∞ –æ —Ä–∞–∑–º–µ—Ä–∞—Ö —Ñ–∞–π–ª–æ–≤ backup'–∞
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ JSON –æ—Ç—á–µ—Ç–∞
"""

import json
import sys
from typing import Dict, List, Tuple


def get_human_size(size_bytes: int) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ –±–∞–π—Ç–∞—Ö –≤ —á–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"""
    if size_bytes == 0:
        return "0B"
    
    size_names = ["B", "KB", "MB", "GB", "TB"]
    size_bytes = float(size_bytes)
    
    i = 0
    while size_bytes >= 1024.0 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    
    return f"{size_bytes:.1f}{size_names[i]}"


def analyze_directory_sizes(directory_data: Dict, path: str = "") -> List[Tuple[str, int, int]]:
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    
    Returns:
        List of tuples (path, included_size, total_size)
    """
    results = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    if path:
        results.append((
            path,
            directory_data.get("included_size_bytes", 0),
            directory_data.get("total_size_bytes", 0)
        ))
    
    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    subdirs = directory_data.get("subdirectories", {})
    for subdir_name, subdir_data in subdirs.items():
        subpath = f"{path}/{subdir_name}" if path else subdir_name
        results.extend(analyze_directory_sizes(subdir_data, subpath))
    
    return results


def find_largest_files(directory_data: Dict, path: str = "", top_n: int = 50) -> List[Tuple[str, str, int, bool]]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç —Å–∞–º—ã–µ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã
    
    Returns:
        List of tuples (file_path, file_name, size, included)
    """
    files = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    for file_info in directory_data.get("files", []):
        if file_info.get("type") == "file" and "size_bytes" in file_info:
            file_path = f"{path}/{file_info['name']}" if path else file_info['name']
            files.append((
                file_path,
                file_info['name'],
                file_info['size_bytes'],
                file_info.get('included', True)
            ))
    
    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    subdirs = directory_data.get("subdirectories", {})
    for subdir_name, subdir_data in subdirs.items():
        subpath = f"{path}/{subdir_name}" if path else subdir_name
        files.extend(find_largest_files(subdir_data, subpath, top_n * 2))  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø
    files.sort(key=lambda x: x[2], reverse=True)
    return files[:top_n]


def analyze_exclusions(directory_data: Dict, path: str = "") -> Dict[str, List]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –∏—Å–∫–ª—é—á–µ–Ω–∏–π
    """
    exclusions = {}
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    for file_info in directory_data.get("files", []):
        if not file_info.get('included', True) and file_info.get('excluded_by_pattern'):
            pattern = file_info['excluded_by_pattern']
            if pattern not in exclusions:
                exclusions[pattern] = []
            
            file_path = f"{path}/{file_info['name']}" if path else file_info['name']
            exclusions[pattern].append({
                'path': file_path,
                'size': file_info.get('size_bytes', 0)
            })
    
    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    subdirs = directory_data.get("subdirectories", {})
    for subdir_name, subdir_data in subdirs.items():
        subpath = f"{path}/{subdir_name}" if path else subdir_name
        sub_exclusions = analyze_exclusions(subdir_data, subpath)
        
        for pattern, files in sub_exclusions.items():
            if pattern not in exclusions:
                exclusions[pattern] = []
            exclusions[pattern].extend(files)
    
    return exclusions


def main():
    if len(sys.argv) != 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python3 analyze_backup_report.py <json_file>")
        return 1
    
    json_file = sys.argv[1]
    
    try:
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ç—á–µ—Ç: {json_file}")
        with open(json_file, 'r', encoding='utf-8') as f:
            report = json.load(f)
        
        print(f"–û—Ç—á–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
        # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        summary = report.get('summary', {})
        print(f"\nüîç –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {summary.get('total_size_human', 'N/A')}")
        print(f"   –†–∞–∑–º–µ—Ä backup: {summary.get('included_size_human', 'N/A')}")
        print(f"   –ò—Å–∫–ª—é—á–µ–Ω–æ: {summary.get('excluded_size_human', 'N/A')} ({summary.get('exclusion_ratio_percent', 0)}%)")
        print(f"   –§–∞–π–ª–æ–≤ –≤ backup: {summary.get('included_files', 0)}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        directory_structure = report.get('directory_structure', {})
        directory_sizes = analyze_directory_sizes(directory_structure)
        directory_sizes.sort(key=lambda x: x[1], reverse=True)  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ included_size
        
        print(f"\nüìÅ –¢–û–ü-20 –î–ò–†–ï–ö–¢–û–†–ò–ô –ü–û –†–ê–ó–ú–ï–†–£ (—á—Ç–æ –ø–æ–ø–∞–¥–∞–µ—Ç –≤ backup):")
        for i, (path, included_size, total_size) in enumerate(directory_sizes[:20], 1):
            excluded_size = total_size - included_size
            print(f"{i:2d}. {path}")
            print(f"    –í backup: {get_human_size(included_size)}")
            if excluded_size > 0:
                print(f"    –ò—Å–∫–ª—é—á–µ–Ω–æ: {get_human_size(excluded_size)}")
            print()
        
        # –ê–Ω–∞–ª–∏–∑ —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        largest_files = find_largest_files(directory_structure)
        
        print(f"\nüìÑ –¢–û–ü-15 –°–ê–ú–´–• –ë–û–õ–¨–®–ò–• –§–ê–ô–õ–û–í:")
        for i, (file_path, file_name, size, included) in enumerate(largest_files[:15], 1):
            status = "‚úÖ –í backup" if included else "‚ùå –ò—Å–∫–ª—é—á–µ–Ω"
            print(f"{i:2d}. {file_name} ({get_human_size(size)}) - {status}")
            print(f"    –ü—É—Ç—å: {file_path}")
            print()
        
        # –ê–Ω–∞–ª–∏–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        exclusions = analyze_exclusions(directory_structure)
        
        print(f"\nüö´ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–°–ö–õ–Æ–ß–ï–ù–ò–ô –ü–û –ü–ê–¢–¢–ï–†–ù–ê–ú:")
        exclusion_stats = []
        for pattern, files in exclusions.items():
            total_size = sum(f['size'] for f in files)
            exclusion_stats.append((pattern, len(files), total_size))
        
        exclusion_stats.sort(key=lambda x: x[2], reverse=True)
        
        for pattern, files_count, total_size in exclusion_stats:
            print(f"   {pattern}: {files_count} —Ñ–∞–π–ª–æ–≤, {get_human_size(total_size)}")
        
        print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())